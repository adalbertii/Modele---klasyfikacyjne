{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZedRj+MXkQrao2p9WqI5v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalbertii/Modele-klasyfikacyjne/blob/main/Embedding_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3D8Zm0MLH15R"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utworzenie prostego modelu z warstwą Embedding\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(input_dim=10,output_dim=4,input_length=2)\n",
        "model.add(embedding_layer)\n",
        "model.compile('adam','mse')\n",
        "\n",
        "# input_dim - rozmiar słownika\n",
        "# output_dim - długość wektora dla  każdego słowa\n",
        "# input_length - maksymalna długość sekwencji\n",
        "\n",
        "# W powyższym przykładzie ustawiamy 10 jako rozmiar słownictwa, ponieważ będziemy kodować liczby od 0 do 9.\n",
        "# Chcemy, aby długość wektora słów wynosiła 4, stąd output_dim jest ustawione na 4.\n",
        "# Długość sekwencji wejściowej do warstwy osadzania będzie wynosić 2"
      ],
      "metadata": {
        "id": "POU5qXfQIKfF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teraz przekażmy przykładowe dane wejściowe do naszego modelu i zobaczmy wyniki.\n",
        "input_data = np.array([[1,2]])\n",
        "pred = model.predict(input_data)\n",
        "print(input_data.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy4pcRPlJXEA",
        "outputId": "316b51ac-0f7a-4708-dbf2-8eec45839be1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 145ms/step\n",
            "(1, 2)\n",
            "[[[-0.02334188  0.03078547  0.01017154  0.04552039]\n",
            "  [ 0.00770851 -0.01431287  0.01345507  0.005794  ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jak widać, każde słowo (1 i 2) jest reprezentowane przez wektor o długości 4. Jeśli wydrukujemy wagi warstwy osadzania, otrzymamy poniższy wynik."
      ],
      "metadata": {
        "id": "40b5QDksJuXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EazLjNyHKFNt",
        "outputId": "5275d63f-0ba1-4eca-d94f-13eaedbe601b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 2, 4)              40        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40 (160.00 Byte)\n",
            "Trainable params: 40 (160.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpa4317yJ1vU",
        "outputId": "5e141673-ce00-4ca7-e475-c430fa9550ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Variable 'embedding/embeddings:0' shape=(10, 4) dtype=float32, numpy=\n",
            "array([[ 0.02532529,  0.02959583, -0.02077184,  0.03307004],\n",
            "       [-0.02334188,  0.03078547,  0.01017154,  0.04552039],\n",
            "       [ 0.00770851, -0.01431287,  0.01345507,  0.005794  ],\n",
            "       [-0.04999755,  0.01052343,  0.035213  , -0.03954636],\n",
            "       [ 0.03153438,  0.0402245 ,  0.04568147, -0.03027884],\n",
            "       [ 0.01681492, -0.01277689, -0.02017144,  0.00512385],\n",
            "       [-0.01597724, -0.00012515, -0.02283381, -0.02160322],\n",
            "       [ 0.04674724,  0.0115404 , -0.02743992,  0.00157394],\n",
            "       [ 0.00702375, -0.00607336,  0.00393804, -0.04824213],\n",
            "       [-0.02383035,  0.01415158,  0.03827745, -0.02038814]],\n",
            "      dtype=float32)>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Te wagi są w zasadzie reprezentacjami wektorowymi słów w słownictwie. Jak omówiliśmy wcześniej, jest to tabela odnośników o rozmiarze 10 x 4, dla słów od 0 do 9. Pierwsze słowo (0) jest reprezentowane przez pierwszy wiersz w tej tabeli, czyli\n",
        "\n",
        "[0.02532529,  0.02959583, -0.02077184,  0.03307004]"
      ],
      "metadata": {
        "id": "MbFD01YXKaGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W tym przykładzie nie wytrenowaliśmy warstwy osadzania. Wagi przypisane do wektorów słów są inicjowane losowo."
      ],
      "metadata": {
        "id": "iiYcMjV0K2U2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SGtqray1K1fP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}